{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI program\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POC env, skip API call\n"
     ]
    }
   ],
   "source": [
    "### Init\n",
    "from databricks_service import DatabricksGenieService\n",
    "\n",
    "# Create an instance of DatabricksService\n",
    "databricks = DatabricksGenieService()\n",
    "\n",
    "# Initialize Databricks service\n",
    "databricks.init_service()\n",
    "\n",
    "# Testing\n",
    "#databricks.query_databricks('Hi test query string SELECT 1 from dual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available():  True\n",
      "model_path = models/Qwen2-0.5B-Instruct/model.safetensors\n",
      "tokenizer_path = models/Qwen2-0.5B-Instruct/tokenizer.json\n",
      "Model found locally. Loading from local directory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "### Init\n",
    "from llm_service import llm_service\n",
    "\n",
    "# Create an instance of LLM service\n",
    "llm_srv = llm_service()\n",
    "\n",
    "# Initialize LLM service\n",
    "llm_srv.init_service()\n",
    "\n",
    "# Testing\n",
    "#print(llm_srv.query_service('How to write SQL to SELECT one record from table dual, provide a SQL is ok.'))\n",
    "#print(llm_srv.query_service('How to write SQL to SELECT distinct name from table dual, provide one SQL is ok.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for processing of chat message\n",
    "def chat_msg_process(service_provider, prompt):\n",
    "    # Here, you would integrate with your databricks or LLM model\n",
    "    print('Get a prompt: ', prompt)\n",
    "    if  \"databricks\" in service_provider.lower():\n",
    "        response_str = databricks.query_service(prompt)\n",
    "    else:\n",
    "        response_str = llm_srv.query_service(prompt)        \n",
    "    return f\"{response_str}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_approval_status():\n",
    "    # Placeholder for approval status \n",
    "    return \"Your data extraction result is submitted and waiting for approval.\"\n",
    "\n",
    "def get_data_extraction_preview():\n",
    "    # Placeholder for actual data extraction result logic\n",
    "    # Return dummy value for POC purpose\n",
    "    dummy_response = \"First 3 rows for your preview: \\n BU, Bond_ID, Market_value\\n HK,B002,1.2m\\n HK,B008,22.8m\\n HK,C082,33.55m\"\n",
    "    response_str = dummy_response\n",
    "    return response_str\n",
    "\n",
    "# Function to handle the submission of chat input\n",
    "def handle_chat(service_provider, chat_input, chat_history):\n",
    "    # Process the chat input with the LLM model\n",
    "    print('Handle_chat() is called.')\n",
    "    \n",
    "    chat_response = chat_msg_process(service_provider, chat_input)\n",
    "    chat_history.append((chat_input, chat_response))\n",
    "    return \"\", chat_history\n",
    "\n",
    "# Function to update the color of status labels\n",
    "def update_lable_color(event):\n",
    "    status_colors = {\"Chat With LLM\": \"gray\", \"Submit Draft Request\": \"gray\", \"Approval Status\": \"gray\", \"Data Extraction Result Preview\": \"gray\"}\n",
    "    status_colors[event] = \"red\"\n",
    "    return [status_colors[label] for label in status_colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI starts\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle_chat() is called.\n",
      "Get a prompt:  How can I extract the bonds and market values (in HK LBU)?\n",
      "POC env, skip API call, return dummy code\n",
      "Handle_chat() is called.\n",
      "Get a prompt:  How to  extract bonds record in HK LBU, list the required fields.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle_chat() is called.\n",
      "Get a prompt:  How to extract bonds record in HK LBU, list the required fields, seperated by comma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/gradio/route_utils.py\", line 276, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/gradio/blocks.py\", line 1897, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/gradio/blocks.py\", line 1483, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/gradio/utils.py\", line 816, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10592/1654281642.py\", line 17, in handle_chat\n",
      "    chat_response = chat_msg_process(service_provider, chat_input)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_10592/3755807448.py\", line 8, in chat_msg_process\n",
      "    response_str = llm_srv.query_service(prompt)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/pgm/phi3vision/selfdataextract/llm_service.py\", line 46, in query_service\n",
      "    outputs = self.model.generate(inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1786, in generate\n",
      "    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "  File \"/home/xiaomai/anaconda3/envs/llamafactory/lib/python3.11/site-packages/transformers/generation/utils.py\", line 1257, in _validate_generated_length\n",
      "    raise ValueError(\n",
      "ValueError: Input length of input_ids is 20, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handle_chat() is called.\n",
      "Get a prompt:  How to extract bonds record in HK LBU, list the required fields.\n"
     ]
    }
   ],
   "source": [
    "print(\"UI starts\")\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(analytics_enabled=False) as demo:\n",
    "    # Top labels with arrows between each\n",
    "    with gr.Row():\n",
    "        chat_with_llm_label = gr.Button(\"Chat With LLM\", variant=\"secondary\")\n",
    "        gr.Markdown('<div style=\"display: flex; align-items: center; justify-content: center; width: 50px;\">&rarr;</div>')\n",
    "        submit_draft_request_label = gr.Button(\"Submit Draft Request\", variant=\"secondary\")\n",
    "        gr.Markdown('<div style=\"display: flex; align-items: center; justify-content: center; width: 50px;\">&rarr;</div>')\n",
    "        approval_status_label = gr.Button(\"Approval Status\", variant=\"secondary\")\n",
    "        gr.Markdown('<div style=\"display: flex; align-items: center; justify-content: center; width: 50px;\">&rarr;</div>')\n",
    "        result_preview_label = gr.Button(\"Data Extraction Result Preview\", variant=\"secondary\")\n",
    "\n",
    "    # Separator line\n",
    "    gr.Markdown(\"---\")\n",
    "    with gr.Row():\n",
    "        service_provider = gr.Radio(choices=[\"Genie (Databricks)\", \"Large Language Model\"], label=\"Select a service\", value=\"Genie (Databricks)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # Chat panel on the left\n",
    "        with gr.Column(scale=2):\n",
    "            chat_output = gr.Chatbot(label=\"Chat Output\")\n",
    "            chat_input = gr.Textbox(label=\"User Input\")\n",
    "            chat_with_llm_botton = gr.Button(\"Chat\")\n",
    "            submit_draft_req_button = gr.Button(\"Submit draft request\")\n",
    "\n",
    "        # Text areas on the right\n",
    "        with gr.Column(scale=1):\n",
    "            approval_status = gr.Textbox(label=\"Approval status\", value='', interactive=False)\n",
    "            result_preview = gr.Textbox(label=\"Data Extraction Results (preview)\", value='', interactive=False)\n",
    "\n",
    "    # Event handling\n",
    "    chat_with_llm_botton.click(handle_chat, inputs=[service_provider,chat_input,chat_output], outputs=[chat_input,chat_output])\n",
    "\n",
    "    # if click the submit draft request botton:\n",
    "    submit_draft_req_button.click(get_approval_status, outputs=[approval_status])\n",
    "    submit_draft_req_button.click(get_data_extraction_preview, outputs=[result_preview])\n",
    "\n",
    "\n",
    "# Launch the Gradio interface\n",
    "demo.launch(share=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamafactory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
